{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rentals and Airbnb data pipeline docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main Pipeline can be found in the src/pipeline.ipynb folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage 1, setting up\n",
    "\n",
    "The pipeline begins by running necessary notebooks.\n",
    "All complex functionality has been places in the src/helpers.ipynb notebook\n",
    "This allows for better readability and process checking in the main pipeline (less cluttering).\n",
    "Also, the helpers can be easily accessed from other notebooks aswell, which allows for better testing.\n",
    "\n",
    "Before actually doing any 'production' transformations and ingestions, critical helpers are checked by running the src/tests/unit_tests notebook. It allows for early detection of any changed functionality that will break our pipeline or result in different than expected results.\n",
    "\n",
    "Please note that some configurations can be edited in the config file (resources/config.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage 2, ingest raw data and stage in bronze\n",
    "\n",
    "This part of the pipeline will load our raw flat files into memory using duckdb.\n",
    "We then stage our input data in our bronze layer (data/output/bronze) as parquet before we make any transformations.\n",
    "We do this in order not to lose any data and to be able to refer back to it at a later moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage 3, clean the raw files and stage in silver\n",
    "\n",
    "This part of the pipeline will do our first transformations on the raw files.\n",
    "We do this in order to be able to extract the needed information from the raw data files\n",
    "We then stage our cleaned data sets in our silver layer (data/output/silver) in order not to lose any data and be able to refer back to it. We are for, for example, able to do some ad hoc analysis on the cleaned data sets, which can be handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage 4, aggregate both datasets and stage in gold\n",
    "\n",
    "This part of the pipeline will extract the needed information from the cleaned data sets. It therefore combines both sets and aggregates on a zipcode level. It then stores the combines dataset in the gold layer (data/output/gold) as the data is now ready to be interpreted by the bussiness on their own (for example with a powerbi connection or something similar). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
